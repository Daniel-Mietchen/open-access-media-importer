#!/usr/bin/env python
# -*- coding: utf-8 -*-

from os import path
from sys import argv, stderr
from urllib2 import urlparse
from time import sleep

import csv
# csv.field_size_limit must be reset according to
# <http://lethain.com/handling-very-large-csv-and-xml-files-in-python/>
csv.field_size_limit(999999999)

import wikitools

from helpers import efetch, template
from model import session, setup_all, create_all, set_source, \
    Article, Journal, SupplementaryMaterial

try:
    action = argv[1]
    target = argv[2]
except IndexError:  # no arguments given
    stderr.write("""
oa-put – Open Access Importer upload operations

usage:  oa-put upload-media [source]

""")
    exit(1)

try:
    assert(action in ['upload-media'])
except AssertionError:  # invalid action
    stderr.write("Unknown action “%s”.\n" % action)
    exit(2)

try:
    exec "from sources import %s as source_module" % target
except ImportError:  # invalid source
    stderr.write("Unknown source “%s”.\n" % target)
    exit(3)

set_source(target)
setup_all(True)

import config

if action == 'upload-media':
    media_refined_directory = config.get_media_refined_source_path(target)
    wiki = wikitools.wiki.Wiki(config.api_url)
    wiki.login(username=config.username, password=config.password)

    def is_uploaded(material):
        params = {
            'action': 'query',
            'list': 'search',
            'srwhat': 'text',
            'srnamespace': '6',  # media files
            'srsearch': '"%s"+"%s"+"%s"' % (
                material.article.title,
                material.label,
                # Mediawiki does not always find text containing parentheses
                material.caption.split('.')[0].split('(')[0]
            )
        }
        request = wikitools.api.APIRequest(wiki, params)
        result = request.query()
        try:
            if result[u'query'][u'searchinfo'][u'totalhits'] > 0:
                return True
        except KeyError:
            if len(result[u'query'][u'search']) > 0:
                return True
        return False

    materials = SupplementaryMaterial.query.filter_by(
        converted=True,
        uploaded=False
    ).all()
    for material in materials:
        url_path = urlparse.urlsplit(material.url).path
        filename = url_path.split('/')[-1] + '.ogv'
        media_refined_path = path.join(media_refined_directory, filename)

        if (path.getsize(media_refined_path) == 0):
            material.converted=False
            continue

        if is_uploaded(material):
            stderr.write("Skipping “%s”, already exists at <%s>.\n" % (
                media_refined_path.encode('utf-8'),
                config.api_url.encode('utf-8')
            ))
            material.uploaded=True
            continue

        article_doi = material.article.doi
        article_pmid = efetch.get_pmid_from_doi(article_doi)
        article_pmcid = efetch.get_pmcid_from_doi(article_doi)
        authors = material.article.contrib_authors
        article_title = material.article.title
        journal_title = material.article.journal.title
        date = material.article.date
        article_url = material.article.url
        license_url = material.article.license_url
        rights_holder = material.article.copyright_holder
        label = material.label
        caption = material.caption
        categories = efetch.get_categories_from_pmid(article_pmid)
        major_category = efetch.get_major_category_from_pmid(article_pmid)

        #TODO: file extension should be adapted for other file formats
        wiki_filename = '.'.join(filename.split('.')[:-2]) + '.ogv'
        if article_title is not None:
            dirty_prefix = article_title
            forbidden_chars = ',;:^/!<>"'
            for character in forbidden_chars:
                dirty_prefix = dirty_prefix.replace(character, '')
            # prefix is first hundred chars of title sans forbidden characters
            prefix = '-'.join(dirty_prefix[:100].split(' '))
            # if original title is longer than cleaned up title, remove last word
            if len(dirty_prefix) > len(prefix):
                prefix = '-'.join(prefix.split('-')[:-1])
            if prefix[-1] != '-':
               prefix += '-'
            wiki_filename = prefix + wiki_filename
        elif major_category is not None:
            wiki_filename = major_category.replace(' ', '-') + '-' + wiki_filename

        page_template = template.page(article_doi, article_pmid, \
            article_pmcid, authors, article_title, journal_title, \
            date, article_url, license_url, rights_holder, label, \
            caption, categories)

        wiki_file = wikitools.wikifile.File(wiki=wiki, title=wiki_filename)
        wiki_file.upload(
            fileobj = open(media_refined_path, 'r'),
            text=page_template.encode('utf-8'),
            comment = 'Uploaded with the Open Access Media Importer. (test edit) [[Commons:Bots/Requests#Open_Access_Media_Importer_Bot_.28talk.C2.A0.C2.B7_contribs.29|botrequest]]'
        )

        stderr.write("“%s” uploaded to <%s>.\n" % (
            media_refined_path.encode('utf-8'),
            config.api_url.encode('utf-8')
        ))

        material.uploaded = True
        session.commit()
        sleep(10)  # 6 uploads per minute
