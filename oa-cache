#!/usr/bin/env python
# -*- coding: utf-8 -*-

from os import listdir, path, remove, rename
from sys import argv, stderr, stdout

import csv
# csv.field_size_limit must be reset according to
# <http://lethain.com/handling-very-large-csv-and-xml-files-in-python/>
csv.field_size_limit(999999999)

import errno
import gobject, pygst
pygst.require("0.10")

import gst
import progressbar

import mutagen.oggtheora

from helpers import media
from model import session, setup_all, create_all, set_source, \
    Article, Journal, SupplementaryMaterial

try:
    action = argv[1]
    target = argv[2]
except IndexError:
    stderr.write("""
oa-cache – Open Access Media Importer local operations

usage:  oa-cache clear-media [source] |
        oa-cache clear-database [source] |
        oa-cache convert-media [source] |
        oa-cache find-media [source] |
        oa-cache list-articles [source]

""")
    exit(1)

try:
    assert(action in ['clear-media', 'clear-database', \
        'convert-media', 'find-media', 'list-articles'])
except AssertionError:  # invalid action
    stderr.write('Unknown action “%s”.\n' % action)
    exit(2)

try:
    exec "from sources import %s as source_module" % target
except ImportError:  # invalid source
    stderr.write("Unknown source “%s”.\n" % target)
    exit(3)

set_source(target)
setup_all(True)

import config

if action == 'clear-media':
    media_raw_directory = config.get_media_refined_source_path(target)
    listing = listdir(media_raw_directory)

    metadata_refined_directory = config.get_metadata_refined_source_path(target)
    download_cache_path = path.join(metadata_refined_directory, 'download_cache')
    remove(download_cache_path)

    for filename in listing:
        media_path = path.join(media_raw_directory, filename)
        stderr.write("Removing “%s” … " % media_path)
        remove(media_path)
        stderr.write("done.\n")

if action == 'clear-database':
    filename = config.database_path(target)
    stderr.write("Removing “%s” … " % filename)
    try:
        remove(filename)
        stderr.write("done.\n")
    except OSError, e:
        stderr.write('\n%s\n' % str(e))

if action == 'convert-media':
    metadata_path = config.get_metadata_refined_source_path(target)
    converted_cache_path = path.join(metadata_path, 'converted_cache')
    download_cache_path = path.join(metadata_path, 'download_cache')
    with open(download_cache_path, 'r') as download_cache:
        with open(converted_cache_path, 'a') as converted_cache:
            reader = csv.reader(download_cache)
            writer = csv.writer(converted_cache)
            for row in reader:
                loop = gobject.MainLoop()

                media_refined_directory = config.get_media_refined_source_path(target)
                temporary_media_path = path.join(media_refined_directory, 'current.ogv')

                media_raw_path = row[14]
                filename = path.split(media_raw_path)[-1]
                media_refined_path = path.join(media_refined_directory, filename + '.ogv')

                if path.isfile(media_refined_path):
                    continue

                stderr.write("Converting “%s”, saving into “%s” …\n" % (
                        media_raw_path,
                        media_refined_path
                    )
                )

                m = media.Media(media_raw_path)
                m.find_streams()
                m.convert(temporary_media_path)

                try:
                    f = mutagen.oggtheora.OggTheora(temporary_media_path)
                    f['TITLE'] = row[9].decode('utf-8')
                    f['ALBUM'] = row[2].decode('utf-8')  # article title
                    f['ARTIST'] = row[1].decode('utf-8')  # authors
                    f['COPYRIGHTS'] = row[8].decode('utf-8')
                    f['LICENSE'] = row[7].decode('utf-8')
                    f['DESCRIPTION'] = row[10].decode('utf-8')
                    f['DATE'] = row[5].decode('utf-8')
                    f.save()
                except mutagen.oggtheora.OggTheoraHeaderError:
                    pass  # Most probably an encoding failure.

                rename(temporary_media_path, media_refined_path)

                row[14] = media_refined_path
                writer.writerow(row)

if action == 'list-articles':
    csv_writer = csv.writer(stdout)
    # categories based on:
    # “Citation Rules with Examples for Journal Articles on the Internet”
    # <http://www.ncbi.nlm.nih.gov/books/NBK7281/#A55596>
    csv_writer.writerow([
        'Authors',
        'Article Title',
        'Article Abstract',  # not part of citation rules, but useful
        'Journal Title',
        'Date of Publication',
        'Available from',
        'License',  # also not part of citation rules
        'Copyright Holder'  # same here
    ])
    for article in Article.query.all():
        dataset = [item.encode('utf-8') for item in
            [
                article.contrib_authors,
                article.title,
                article.abstract,
                article.journal.title,
                article.date,
                article.url,
                article.license_url,
                article.copyright_holder
            ] if 'encode' in dir(item)
        ]
        try:
            csv_writer.writerow(dataset)
        except IOError, e:
            if e.errno == errno.EPIPE:
                exit(0)  # broken pipe, exit normally
            else:
                raise

if action == 'find-media':
    skip = [article.name for article in Article.query.all()]
    if len(skip) > 0:
        stderr.write('Skipping %s records … \n' % len(skip))
    source_path = config.get_metadata_raw_source_path(target)    
    for result in source_module.list_articles(
        source_path,
        supplementary_materials=True,
        skip=skip
    ):
        journal = Journal.get_by(title=result['journal-title'])
        if not journal:
            journal = Journal(
                title = result['journal-title']
            )
        article = Article.get_by(
            title=result['article-title'],
            contrib_authors=result['article-contrib-authors']
        )
        if not article:
            article = Article(
                name=result['name'],
                title=result['article-title'],
                contrib_authors=result['article-contrib-authors'],
                abstrict=result['article-abstract'],
                date=result['article-date'],
                url=result['article-url'],
                license_url=result['article-license-url'],
                copyright_holder=result['article-copyright-holder'],
                journal=journal
            )
        materials = result['supplementary-materials']
        if materials:
            stderr.write(
                '%d supplementary materials in “%s”:\n\t' %
                (
                    len(result['supplementary-materials']),
                    result['article-title'].encode('utf-8')
                )
            )
            for material in materials:
                stderr.write(
                    '%s/%s ' % (
                        material['mimetype'],
                        material['mime-subtype']
                    )
                )
                supplementary_material = SupplementaryMaterial.get_by(url=material['url'])
                if not supplementary_material:
                    supplementary_material=SupplementaryMaterial(
                        label=material['label'],
                        caption=material['caption'],
                        mimetype=material['mimetype'],
                        mime_subtype=material['mime-subtype'],
                        url=material['url'],
                        article=article
                    )
            session.commit()
            stderr.write('\n')
