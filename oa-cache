#!/usr/bin/env python
# -*- coding: utf-8 -*-

from os import listdir, path, remove, rename
from sys import argv, stderr, stdout
from urllib2 import urlparse

import csv
# csv.field_size_limit must be reset according to
# <http://lethain.com/handling-very-large-csv-and-xml-files-in-python/>
csv.field_size_limit(999999999)

import errno
import gobject, pygst
pygst.require("0.10")

import gst
import progressbar

import mutagen.oggtheora

import pprint

from helpers import media
from model import session, setup_all, create_all, set_source, \
    Article, Journal, SupplementaryMaterial

try:
    action = argv[1]
    target = argv[2]
except IndexError:
    stderr.write("""
oa-cache – Open Access Media Importer local operations

usage:  oa-cache clear-media [source] |
        oa-cache clear-database [source] |
        oa-cache convert-media [source] |
        oa-cache find-media [source] |
        oa-cache list-articles [source] |
        oa-cache stats [source]

""")
    exit(1)

try:
    assert(action in ['clear-media', 'clear-database', \
        'convert-media', 'find-media', 'list-articles', 'stats'])
except AssertionError:  # invalid action
    stderr.write('Unknown action “%s”.\n' % action)
    exit(2)

try:
    exec "from sources import %s as source_module" % target
except ImportError:  # invalid source
    stderr.write("Unknown source “%s”.\n" % target)
    exit(3)

set_source(target)
setup_all(True)

import config

if action == 'clear-media':
    media_raw_directory = config.get_media_refined_source_path(target)
    listing = listdir(media_raw_directory)

    metadata_refined_directory = config.get_metadata_refined_source_path(target)
    download_cache_path = path.join(metadata_refined_directory, 'download_cache')
    remove(download_cache_path)

    for filename in listing:
        media_path = path.join(media_raw_directory, filename)
        stderr.write("Removing “%s” … " % media_path)
        remove(media_path)
        stderr.write("done.\n")

if action == 'clear-database':
    filename = config.database_path(target)
    stderr.write("Removing “%s” … " % filename)
    try:
        remove(filename)
        stderr.write("done.\n")
    except OSError, e:
        stderr.write('\n%s\n' % str(e))

if action == 'convert-media':
    materials = SupplementaryMaterial.query.filter_by(
        downloaded=True,
        converted=False
    ).all()
    for material in materials:
        media_refined_directory = config.get_media_refined_source_path(target)
        media_raw_directory = config.get_media_raw_source_path(target)
        temporary_media_path = path.join(media_refined_directory, 'current.ogv')

        url_path = urlparse.urlsplit(material.url).path
        filename = url_path.split('/')[-1]
        media_raw_path = path.join(media_raw_directory, filename)
        media_refined_path = path.join(media_refined_directory, filename + '.ogv')

        if path.isfile(media_refined_path):
            material.converted = True
            session.commit()
            continue

        stderr.write("Converting “%s”, saving into “%s” …\n" % (
                media_raw_path.encode('utf-8'),
                media_refined_path.encode('utf-8')
            )
        )

        loop = gobject.MainLoop()
        m = media.Media(media_raw_path)
        m.find_streams()
        m.convert(temporary_media_path)

        try:
            f = mutagen.oggtheora.OggTheora(temporary_media_path)
            f['TITLE'] = material.label
            f['ALBUM'] = material.article.title
            f['ARTIST'] = material.article.contrib_authors
            f['COPYRIGHTS'] = material.article.copyright_holder
            f['LICENSE'] = material.article.license_url
            f['DESCRIPTION'] = material.caption
            f['DATE'] = material.article.date
            f.save()
        except mutagen.oggtheora.OggTheoraHeaderError:
            pass  # Most probably an encoding failure.

        rename(temporary_media_path, media_refined_path)

        material.converted = True
        session.commit()

if action == 'list-articles':
    csv_writer = csv.writer(stdout)
    # categories based on:
    # “Citation Rules with Examples for Journal Articles on the Internet”
    # <http://www.ncbi.nlm.nih.gov/books/NBK7281/#A55596>
    csv_writer.writerow([
        'Authors',
        'Article Title',
        'Article Abstract',  # not part of citation rules, but useful
        'Journal Title',
        'Date of Publication',
        'Available from',
        'License',  # also not part of citation rules
        'Copyright Holder'  # same here
    ])
    for article in Article.query.all():
        dataset = [item.encode('utf-8') for item in
            [
                article.contrib_authors,
                article.title,
                article.abstract,
                article.journal.title,
                article.date,
                article.url,
                article.license_url,
                article.copyright_holder
            ] if 'encode' in dir(item)
        ]
        try:
            csv_writer.writerow(dataset)
        except IOError, e:
            if e.errno == errno.EPIPE:
                exit(0)  # broken pipe, exit normally
            else:
                raise

if action == 'find-media':
    skip = [article.name for article in Article.query.all()]
    if len(skip) > 0:
        stderr.write('Skipping %s records … \n' % len(skip))
    source_path = config.get_metadata_raw_source_path(target)
    for result in source_module.list_articles(
        source_path,
        supplementary_materials=True,
        skip=skip
    ):
        try:
            journal = Journal.get_by(title=result['journal-title'])
            if not journal:
                journal = Journal(
                    title = result['journal-title']
                )
            article = Article.get_by(
                title=result['article-title'],
                contrib_authors=result['article-contrib-authors']
            )
            if not article:
                article = Article(
                    name=result['name'],
                    doi=result['doi'],
                    title=result['article-title'],
                    contrib_authors=result['article-contrib-authors'],
                    abstract=result['article-abstract'],
                    date=result['article-date'],
                    url=result['article-url'],
                    license_url=result['article-license-url'],
                    copyright_holder=result['article-copyright-holder'],
                    journal=journal
                )
            materials = result['supplementary-materials']
            if materials:
                stderr.write(
                    '%d materials in “%s”:\n\t' %
                    (
                        len(result['supplementary-materials']),
                        result['article-title'].encode('utf-8')
                    )
                )
                mimetypes = {}
                for material in materials:
                    mimetype = material['mimetype'] + '/' + material['mime-subtype']
                    try:
                        mimetypes[mimetype] += 1
                    except KeyError:
                        mimetypes[mimetype] = 1
                    supplementary_material = SupplementaryMaterial.get_by(url=material['url'])
                    if not supplementary_material:
                        supplementary_material=SupplementaryMaterial(
                            label=material['label'],
                            caption=material['caption'],
                            mimetype=material['mimetype'],
                            mime_subtype=material['mime-subtype'],
                            url=material['url'],
                            article=article
                        )
                for mimetype in mimetypes.keys():
                     stderr.write(
                        '%s: %s ' % (
                            mimetypes[mimetype],
                            mimetype
                            )
                        )
                stderr.write('\n')
                session.commit()
        except KeyboardInterrupt:
            stderr.write('Saving database …\n')
            session.commit()
            exit(0)

if action == "stats":
    stderr.write('Counting supplementary materials … ')
    materials = SupplementaryMaterial.query.all()
    stderr.write(str(len(materials)) + ' supplementary materials found.\n')
    p = progressbar.ProgressBar(maxval=len(materials))
    completed = 0
    licenses = {
        'free': {},
        'non-free': {}
    }
    mimetypes = {
        'free': {},
        'non-free': {}
    }
    for material in materials:
        license_url = material.article.license_url
        mimetype = material.mimetype
        mime_subtype = material.mime_subtype
        mimetype_composite = mimetype + '/' + mime_subtype
        if license_url in config.free_license_urls:
            try:
                licenses['free'][license_url] += 1
            except KeyError:
                licenses['free'][license_url] = 1
            try:
                mimetypes['free'][mimetype_composite] += 1
            except KeyError:
                mimetypes['free'][mimetype_composite] = 1
        else:
            try:
                licenses['non-free'][license_url] += 1
            except KeyError:
                licenses['non-free'][license_url] = 1
            try:
                mimetypes['non-free'][mimetype_composite] += 1
            except KeyError:
                mimetypes['non-free'][mimetype_composite] = 1
        completed += 1
        p.update(completed)

    stderr.write('\n')
    pp = pprint.PrettyPrinter(indent=4)
    output = pp.pformat({
        'licenses': licenses,
        'mimetypes': mimetypes
    })
    stdout.write(output + '\n')
